training: !!bool "true"
device: "mps"
unprocessed_data: '/Users/bapt/Desktop/ENSAE/3ème Année/Advanced Machine Learning/Datasets/LibriSpeech/train-clean-360/*/*'
---
data:
    train_path: '/Users/bapt/Desktop/ENSAE/3ème Année/Advanced Machine Learning/Datasets/train'
    test_path: '/Users/bapt/Desktop/ENSAE/3ème Année/Advanced Machine Learning/Datasets/test'
    sr: 16000
    nfft: 512 #For mel spectrogram preprocess
    window: 0.025 #(s)
    hop: 0.01 #(s)
    nmels: 40 #Number of mel energies
    tisv_frame: 160 #Number of frames as input for the log-mel-spectrogram
---   
model:
    hidden: 768 #Number of LSTM hidden layer units
    nb_layers: 3 #Number of LSTM layers
    proj: 64 #Embedding size
    model_path: '/Users/bapt/Desktop/ENSAE/3ème Année/Advanced Machine Learning/Models/Encoder' #Model path for testing, inference, or resuming training
---
train:
    N : 6 #Number of speakers in batch
    M : 4 #Number of utterances per speaker
    num_workers: 0 #number of workers for dataloader
    ratio: 0.9 #Ratio of train speakers
    lr: 0.01 
    epochs: 800 #Max training speaker epoch 
    log_interval: 30 #Epochs before printing progress
    checkpoint_interval: 120 #Save model after x speaker epochs
    checkpoint_dir: '/Users/bapt/Desktop/ENSAE/3ème Année/Advanced Machine Learning/Model/checkpoints'
    restore: !!bool "false" #Resume training from previous model path
---
test:
    N : 10 #Number of speakers in batch
    M : 20 #Number of utterances per speaker
    enroll_ratio: 0.1 #Ratio of enroll speakers
    num_workers: 8 #number of workers for data laoder
    epochs: 10 #testing speaker epochs